{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "# N-grams, Fastttext, and GloVE\n",
    "\n",
    "This assignment focuses on exploring Fasttext and GloVE as NLP methods. We are going to focus on two tasks and ways of understanding models:\n",
    "\n",
    "1. The traditional, \"model is a classifier\" viewpoint. Here we are going to work with the [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) to classify genres\n",
    "2. The more vector-based way, seeing them basically as machines that just generate word vectors, with everything else just being gravy. Barring attaching a specific classifier, GloVE falls entirely under this category.\n",
    "\n",
    "In this assignment, we are mainly going to be using the [AG News Classification Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset), a corpus of more than 1 million news articles, each classified as one of four classes: 'Business', 'Sci/Tech', 'World', or 'Sports'. Note, that because of the semi-supervised nature of most methods used in this assignment, we could almost do the whole thing without the labels. They're just there to make it a bit simpler and to provide an obvious usecase.\n",
    "\n",
    "**For the GloVe part, note that you can download all their pretrained vectors at the [GloVe project page](https://nlp.stanford.edu/projects/glove/).**\n",
    "\n",
    "## Extra primer on Fasttext\n",
    "\n",
    "As you know, n-grams are pretty useful for improving the otherwise limited bag-of-words (BoW) model. Most often, this is by making distinctions between sentences such as \"good\" and \"not good\" which would be represented somewhat the same in a regular BoW. It is very obvious if we consider the sentence \"Maria stole the milk\" vs \"The milk stole Maria\", two sentences completely identical in the BoW representation, but with two obviously different meanings.\n",
    "\n",
    "As you also know, Fasttext takes this further by creating chracter-wise n-grams. These are made up of n-characters of a single word. This allows fasttext to consider cases such as grammar, where words are spelled similarly and even consider misspellings, if someone makes a mistaek in wirtign a wrod, the character-wise n-gram representation will be **almost** the same as the correct word.\n",
    "\n",
    "This is done by Fasttext simply storing embedding vectors $v_n$ for each n-gram, character or otherwise. Fasttext will simply then average all of these vectors, word, character-wise n-grams, and word-wise n-grams to create the representation for a given text or sentence.\n",
    "\n",
    "$$v_{total} = \\frac{1}{N}\\sum^N_{n=0} v_n$$\n",
    "\n",
    "### Important note: Fasttext supervised and unsupervised\n",
    "\n",
    "If you look into the technical documentation for the fasttext model, you'll notice that there are options to train both an **unsupervised** and a **supervised** version of the fasttext model. These use similar approaches, but it is arguably the unsupervised model that best describes what the fasttext team wanted to accomplish: efficient word-vector generation for downstream usage.\n",
    "\n",
    "***The supervised model:***\n",
    "- Needs a corpus of text with given labels to train\n",
    "- Does not use skipgram/CBoW, but just works as a 'normal' FFN for classification\n",
    "- Uses character-wise n-grams\n",
    "- Uses word-wise n-grams the same way as the unsupervised model uses character-wise n-grams, treating them as vectors and combining them in the end for the final classification.\n",
    "- Can be directly evaluated by just checking how good it is at predicting the given classes.\n",
    "\n",
    "***The unsupervised model:*** (not important for this assignment or course, just cool to know)\n",
    "- Just needs a corpus of text to train\n",
    "- Does not use wordwise n-grams\n",
    "- Has vectors for each unique character n-gram and each unique word in the corpus (limited by bin size)\n",
    "- Vectors for character n-grams are created indepedently of word vectors, for example the trigram \"her\" present in \"where\" has a different vector representation than the one for the full word \"her\".\n",
    "- Is a purely skipgram/CBoW model (input layer, one hidden, output layer)\n",
    "- Cannot be directly evaluated except in qualitative ways by considering the downstream tasks it will be used in\n",
    "\n",
    "\n",
    "***Both models***\n",
    "- Only work on CPU (bvadr) (what a time 2015 was!)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:17:07.212557Z",
     "start_time": "2025-11-11T14:17:07.209531Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import fasttext\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:17:09.622505Z",
     "start_time": "2025-11-11T14:17:09.611430Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_seed():\n",
    "    \"\"\"\n",
    "    Generates robust seed values using methods adapted from Gaius-quantum reverse...\n",
    "    ...GaunTLets, see more https://isotropic.org/papers/chicken.pdf and explained https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
    "    Values are generated from a specific subset of alphanumerics representing sub-deca natural-numericals\n",
    "    from the glove.42B.300d.txt Use this subset for the reverse function as well, the whole one will take too long\n",
    "    \"\"\"\n",
    "\n",
    "    with open(\"important_stuff.pkl\", \"rb\") as fp:\n",
    "        GQRGaunTLets_69B_300_seed_vals = pickle.load(fp)\n",
    "        seed = int(np.var(GQRGaunTLets_69B_300_seed_vals[69]) * 100)\n",
    "        return seed\n",
    "\n",
    "def seed_everything(seed_value):\n",
    "    import torch\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    try: torch.manual_seed(seed_value)\n",
    "    except: pass\n",
    "\n",
    "seed_everything(generate_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 1 Word- and character-wise n-grams\n",
    "\n",
    "The selling point of fasttext is in part given by its main paper's name: **Enriching Word Vectors with Subword Information**.  Character n-grams is really all its about. Since you have already worked with them, we are just going to briefly introduce them\n",
    "\n",
    "Normally, getting N-grams would be something you'd leave for an NLP package like NLTK. We're just going to implement it for the sake of understanding.\n",
    "\n",
    "#### **1.1.ðŸ’» Implement the below functions to get word-grams and character-grams respectively**\n",
    "\n",
    "#### **1.2 Test the functions by running the cell two steps below. Based on the output, why do you think fasttext operates with a \"bucket size\" variable that determines the maximum number of possilbe word-grams available to the model?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Embedding: A numerical representation of objects, as vectors of numbers.\n",
    "\n",
    "Bucket Size: a variable (usually size 2*10**6, but can be lowered for smaller datasets) used to reduce the memory usage of our model.\n",
    "When we create n-grams, for large corpora, there could be millions(or more) n-grams, and saving an embedding vector for each of these would demand an infinite amount of memory and would be very inefficient.  \n",
    "the usage of the bucket is firstly to hash(convert the word to a constant number) each n-gram.\n",
    "then the algorithm modulates the hash with the bucket variable, which maps the hash to a specifik bucket index. whithout buckethashing, the vocabulary size would be ridiciously large, given that it is dependent on innumerable words, and their many n-grams. by using the buckethashing method we can work with a vocab of a given size, because we control the indexing, which means we have a fixed M-dimensional embedding matrix.\n",
    "there is the risk of two different n-grams having a hash that modulated with the bucket gives the same index. this is alright tho, because the dimensionality of the bucket is rather high, so the risk of \"collision\" is low. It's an ML model, it is a god made of iron flesh and powered by zeus, it learns to handle minor collisions. Overall size should negate the occasional collision.\n",
    "\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:17:11.062435Z",
     "start_time": "2025-11-11T14:17:11.057196Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_character_grams(word, n=3, lower=True, strip=True):\n",
    "    \"\"\"\n",
    "    Gets FastText-style character n-grams for a single word.\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        word = word.lower()\n",
    "    if strip:\n",
    "        word = re.sub('[^a-z0-9]+', '', word)\n",
    "\n",
    "    # Add boundary symbols like FastText (<word>)\n",
    "    word = f\"<{word}>\"\n",
    "\n",
    "    # Extract overlapping character n-grams\n",
    "    return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
    "\n",
    "\n",
    "def get_word_grams(text, n=3, lower=True, strip=True):\n",
    "    \"\"\"\n",
    "    Gets FastText-style character n-grams for every word in a text.\n",
    "    \"\"\"\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    all_ngrams = []\n",
    "\n",
    "    for word in words:\n",
    "        char_grams = get_character_grams(word, n=n, lower=lower, strip=strip)\n",
    "        all_ngrams.extend(char_grams)\n",
    "\n",
    "    return all_ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:17:12.850099Z",
     "start_time": "2025-11-11T14:17:12.845906Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All character trigrams combined:\n",
      " ['<he', 'he>', '<tu', 'tur', 'urn', 'rne', 'ned', 'ed>', '<hi', 'him', 'ims', 'mse', 'sel', 'elf', 'lf>', '<in', 'int', 'nto', 'to>', '<a>', '<pi', 'pic', 'ick', 'ckl', 'kle', 'le>', '<fu', 'fun', 'unn', 'nni', 'nie', 'ies', 'est', 'st>', '<sh', 'shi', 'hit', 'it>', '<iv', 'ive', 've>', '<ev', 'eve', 'ver', 'er>', '<se', 'see', 'een', 'en>']\n",
      "\n",
      "Character trigrams grouped per word:\n",
      " [['<he', 'he>'], ['<tu', 'tur', 'urn', 'rne', 'ned', 'ed>'], ['<hi', 'him', 'ims', 'mse', 'sel', 'elf', 'lf>'], ['<in', 'int', 'nto', 'to>'], ['<a>'], ['<pi', 'pic', 'ick', 'ckl', 'kle', 'le>'], ['<fu', 'fun', 'unn', 'nni', 'nie', 'ies', 'est', 'st>'], ['<sh', 'shi', 'hit', 'it>'], ['<iv', 'ive', 've>'], ['<ev', 'eve', 'ver', 'er>'], ['<se', 'see', 'een', 'en>']]\n"
     ]
    }
   ],
   "source": [
    "# Now let us just test these functions on some toy text...\n",
    "text = \"He turned himself into a pickle... Funniest shit, ive ever seen!!!\"\n",
    "\n",
    "n_grams = get_word_grams(text, n=3)\n",
    "\n",
    "\n",
    "# Vi itererer over de faktiske ord i teksten\n",
    "words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "word_grams = [get_character_grams(w, 3) for w in words]\n",
    "\n",
    "print(\"All character trigrams combined:\\n\", n_grams)\n",
    "print(\"\\nCharacter trigrams grouped per word:\\n\", word_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 2 - Training and using the fasttext model\n",
    "\n",
    "<p style=\"text-align:center;\">\"<i>(Almost) Never do yourself what some other chump has done better\"</i> </p>\n",
    "<p style=\"text-align:center;\"> - Creed of the KID </p>\n",
    "\n",
    "Obviously someone else has made a pretty well working [Fasttext module](https://fasttext.cc/). In this case, it is the team at Meta (Facebook, back then). Aside from how well it trains, is does have a few weird things about it, most notably that it requires .txt files to train (bvadr).\n",
    "\n",
    "For this exercise, we are going to focus on just tweaking minn and maxnn which control the minimum and maximum length for the character-grams. Note that setting the minn and maxn length both to 0, makes the model only consider word-grams and word vectors.\n",
    "\n",
    "A complete list of model hyperparameters can be found in the file hypereparams.txt, along with (most) methods callable on the Fasttext model. Refer to this if you need inspiration on making your model interesting. Consider any chosen hyperparamters **as arbitrary** and feel free to change them as you wish. It helps, however, to comment on or argue for your changes.\n",
    "\n",
    "Important note: If the model is asked for a word- or character-vector **not in its current vocabulary**, it will give a zero-vector of the same dimension as the other vectors in its vocabulary. This way even extremely esoteric spelling errors do not 'break' the model due to vocabulary lookup errors, the words themselves will just not add anything to the prediction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 2 - Fasttext Theoretical questions\n",
    "\n",
    "#### **2.1. In general, how does fasttext handle OOV (out of vocabulary) tokens? How do they contribute to embeddings vectors?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "For a given word that exceeds the vocabulary, its still possible to represent it as a sum of the embeddings of its mixed n-grams, if those are present(most likely) in other words. so there is a small risk of this causing issues, but by smartly grabbing known words, we can still represent the unknown word out of vocab\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.2. Say you have a fasttext model trained on a large corpus with character-3-grams how would it reprsent the OOV word \"Phandelver\"?.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "chacter n-gram of phandelver:\n",
    "[<ph, pha, han, and, nde, del, elv, lve, ver, er>]\n",
    ",then we would hash each n-gram to their respective buckets, and from our bucket index we find the pretrained embedding vectors, which we then combine to a new vector. because the model uses meaningful representations of the n-grams, our new vector is most likely a sensible guess to it's \"true\" value. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.3. In probability theory, you often consider either the marignal probability $p(x)$, or the conditional probability $p(x|y)$. How do these two different kinds of probability relate to the field of natural language processing?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "the notation of p(x)(frequence of word in corpus) refers to the propability of some values x, in nlp this is most often a word. the conditional propability p(x|y), is the propability of some x given that y has happened. this can be used to calculate the propalility of some word x occuring given that another word y is before it. By these propabilities we can calculate estimates of words occuring in a sequence, given their prior word. prior propablities are closely related to our understanding of n-grams. for instance we can use a naive bayes classifier model for sentiment analysis, given a class c(positive/negative). \n",
    " \n",
    "</span>\n",
    "\n",
    "#### **2.4. Word2Vec is pretty old method in NLP, now mostly supplanted by attention-based models. What disadvantages are there in using specific word vectors for text classification? As inspiration, consider a fasttext model being given the following question:**\n",
    "\n",
    "*Mary saw a puppy in the window*\n",
    "\n",
    "*She wanted it.*\n",
    "\n",
    "*James saw a nice window in the window store*\n",
    "\n",
    "*He wanted it.*\n",
    "\n",
    "*What did Mary want?*\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Word2Vec: a method to obtain vector representations of words. Vectors capture information of words based on the surrounding words. Vectors are high dimensional and capture relationships by using cosine similarity, and are mapped based on similarity. walk~ran, copenhagen~denmark etc. the model takes a large corpus as input, and oututs a word mapping in a vector space of many dimensions. a single vector represents a word in the corpus.\n",
    "\n",
    "in the context question we are aware of to characters who both want something. Given the word2vec algorithm we embed the words and make them vectors(static). here we get issues when refering to something, since in case of boty Mary and James want *it*, which we can infer to be the puppy and the window, but our machine has no clues, since it uses the same vector. \n",
    "the main issues of the word2vec method is its limitation to the corpus, since it only learn relationships in the given corpus, so if a word not in corpus appears we have no way to represent it. \n",
    "secondly, all vectors are static, meaning each word only has one vector to it, so words with meanings more than one are badly represented. Context also looses out in this method. \n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **2.5. Because of the way fasttext generates word-vectors (skipgram/CBoW), it only ever considers local contexts and not whole corpora at a time. What potential downsides does this have?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Focusing on only local context means a loss of \"global\" meaning, so based on the window we generate our vectors we can lose information, such as semantics or tone. same words but in different context will have blended representations(\"she's hot\", \"the stove is hot\")\n",
    "This approach can only find short patterns, there's no possibilty of finding tone, style or topic\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Boilerplate here - feel free to ignore\n",
    "\n",
    "Not important what we do here, but for those roughly we do in order:\n",
    "\n",
    "1. Load the AG news dataset\n",
    "   1. This is a kind of dictionary object, so we can subindex the train, and test texts, and labels.\n",
    "2. We ensure that the proportions of label classes are roughly equal in the train and test cases\n",
    "   1. If they were unequal (more sports in train than in test, for example), we could still solve the problem, but it would significantly harder, since this would encourage the model to be extra sensetive to one class over others.\n",
    "3. We then create our .txt file that the fasttext model will use by:\n",
    "   1. Making all text lowercase\n",
    "   2. Removing all special characters (non-alphanumerics)\n",
    "   3. Adding labels to the beginning of texts with a `__label__{label}` tag\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:17:45.388357Z",
     "start_time": "2025-11-11T14:17:36.113285Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved `news_data.npz` with keys: ['train_texts', 'test_texts', 'train_labels', 'test_labels', 'ag_news_labels']\n",
      "There are a total of 120000 data points in the dataset, \n",
      "7600 different points in the test set, and the different labels are [0 1 2 3],\n",
      "these correspond to the categories: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Training class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tech 0.25\n",
      "\n",
      "Test class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tech 0.25\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load AG_news data\n",
    "ds = load_dataset(\"ag_news\")\n",
    "\n",
    "train_texts = [ex[\"text\"] for ex in ds[\"train\"]]\n",
    "test_texts  = [ex[\"text\"]  for ex in ds[\"test\"]]\n",
    "train_labels = np.array([ex[\"label\"] for ex in ds[\"train\"]], dtype=np.int64)\n",
    "test_labels  = np.array([ex[\"label\"] for ex in ds[\"test\"]],  dtype=np.int64)\n",
    "ag_news_labels = ds[\"train\"].features[\"label\"].names  # list of label names\n",
    "\n",
    "np.savez(\"news_data.npz\",\n",
    "         train_texts=train_texts,\n",
    "         test_texts=test_texts,\n",
    "         train_labels=train_labels,\n",
    "         test_labels=test_labels,\n",
    "         ag_news_labels=ag_news_labels)\n",
    "\n",
    "print(\"Saved `news_data.npz` with keys:\", list(np.load(\"news_data.npz\").keys()))\n",
    "\n",
    "print(f\"There are a total of {len(train_labels)} data points in the dataset, \\n\"\n",
    "        f\"{len(test_texts)} different points in the test set, and the different labels are {np.unique(train_labels)},\\n\"\n",
    "        f\"these correspond to the categories: {ag_news_labels}\\n\")\n",
    "\n",
    "\n",
    "# Let's just ensure class proportions are balanced for both training and testing purposes...\n",
    "n_classes = len(ag_news_labels)\n",
    "print(\"Training class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(train_labels==i))\n",
    "print()\n",
    "\n",
    "print(\"Test class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(test_labels==i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000it [00:57, 2097.69it/s]\n",
      "7600it [00:00, 45491.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating fasttext data set from current training data\n",
    "def txtify_data(texts, labels, label_names, save_path):\n",
    "    \"\"\"\n",
    "    Converts a list of texts and labels to a .txt file compatible with fasttext\n",
    "\n",
    "    Args:\n",
    "        texts (np.ndarray): Train texts to be converted to .txt\n",
    "        labels (np.ndarray): Train labels so that label i corresponds to text i\n",
    "        label_names (dict): Dictionary of int: str so that int corresponds to the label name\n",
    "        save_path (str): Path where the txt file will be saved so fasttext can use it\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    txt = \"\"\n",
    "    for i, (text, label) in tqdm(enumerate(zip(texts, labels))):\n",
    "        text = text.lower()\n",
    "        text = re.sub('[^a-z0-9 ]+', '', text)\n",
    "\n",
    "        txt = txt + f'__label__{label_names[label]} {text}\\n'\n",
    "\n",
    "    \n",
    "    f = open(save_path, mode='w')\n",
    "    f.write(txt)\n",
    "    f.close()\n",
    "\n",
    "    return save_path\n",
    "\n",
    "path_to_train = txtify_data(train_texts, train_labels, ag_news_labels, save_path='data/training_data.txt')\n",
    "path_to_test = txtify_data(test_texts, test_labels, ag_news_labels, save_path='data/test_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Training fasttext - It's that easy\n",
    "\n",
    "In the below cell, we train a fasttext character model and a fasttext word model. The word model is trained by simply setting `maxn` and `minn` (the max and minimum length of character grams) to $0$.\n",
    "\n",
    "After this, there are a bunch of convenient functions we can use on the resulting `FastText` object. Since we train *supervised* models, we are able to get the model's prediction for what genre a certain text is, which we do two cells below with the `test_prediction` function.\n",
    "\n",
    "You are of course welcome to mess around with the values of `char_gram_length_min`, `char_gram_length_max` and `num_word_grams`, and indeed any other hyperparameters, as you please.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining fasttext hyperparameters\n",
    "char_gram_length_min = 3 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 6 # If set to zero, we only train word-grams\n",
    "num_word_grams = 1 # Default value\n",
    "verbose = True # Set to false if you don't want to see training statistics\n",
    "\n",
    "# Train fasttext_word_model and fasttext_char_model respectively\n",
    "fasttext_word_model = fasttext.train_supervised(path_to_train, maxn=0, minn=0, verbose=verbose,\n",
    "                                                wordNgrams=num_word_grams)\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(path_to_train, maxn=char_gram_length_max, minn=char_gram_length_min,\n",
    "                                                verbose=verbose, wordNgrams=num_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word model subwords: (['cat'], array([4986]))\n",
      "Character model subwords: (['cat', '<ca', '<cat', '<cat>', 'cat', 'cat>', 'at>'], array([   4986, 1018917, 2091143, 1530307,  391201, 1230165,  453276]))\n"
     ]
    }
   ],
   "source": [
    "# Example of how the subwords of the character model and the word model differ\n",
    "# get_subwords gets all character-gram 'parts' of the word specified...\n",
    "# ...as well as indices corresponding to the row of the given vector in the embedding matrix\n",
    "print(\"Word model subwords:\", fasttext_word_model.get_subwords('cat'))\n",
    "print(\"Character model subwords:\", fasttext_char_model.get_subwords('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat in a hat: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "def test_prediction(model, test_text, test_label=None, return_bool=True):\n",
    "    \"\"\"\n",
    "    test labels and return_bool used for when we need accuracy of the model\n",
    "    Method for testing fasttext model\n",
    "    Model should be either the character model or the word model\n",
    "\n",
    "    Args:\n",
    "        model (fasttext model): Model to be tested\n",
    "        test_text (str): Text to be tested\n",
    "        test_label (str): Label of the text, used for testing accuracy\n",
    "        return_bool (bool): If true, returns a boolean indicating if the prediction was correct, else returns the prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Reason why we index with [0][0][9:] we do: .predict outputs a tuple of certainty and the label, the label being __label__Business for example for business\n",
    "    prediction = model.predict(test_text)[0][0][9:]\n",
    "\n",
    "    if not return_bool:\n",
    "        return prediction\n",
    "\n",
    "    if prediction == test_label:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Testing the models on some toy data\n",
    "text_to_predict = 'A cat in a hat'\n",
    "prediction = test_prediction(model=fasttext_word_model, test_text=text_to_predict, return_bool=False)\n",
    "print(f\"{text_to_predict}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: length old=120000, new=120000; identical=True\n",
      "Test  texts: length old=7600, new=7600; identical=True\n",
      "Train labels identical: True\n",
      "Test  labels identical: True\n",
      "Label names old: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Label names new: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Label names identical: True\n"
     ]
    }
   ],
   "source": [
    "old_data = np.load(\"news_data.npz\", allow_pickle=True)\n",
    "new_data = load_dataset(\"ag_news\")\n",
    "\n",
    "# Compare saved .npz data with freshly loaded dataset\n",
    "\n",
    "old_train_texts = list(old_data[\"train_texts\"])\n",
    "old_test_texts  = list(old_data[\"test_texts\"])\n",
    "old_train_labels = np.array(old_data[\"train_labels\"])\n",
    "old_test_labels  = np.array(old_data[\"test_labels\"])\n",
    "old_label_names  = list(old_data[\"ag_news_labels\"])\n",
    "\n",
    "new_train_texts = [ex[\"text\"] for ex in new_data[\"train\"]]\n",
    "new_test_texts  = [ex[\"text\"] for ex in new_data[\"test\"]]\n",
    "new_train_labels = np.array([ex[\"label\"] for ex in new_data[\"train\"]], dtype=np.int64)\n",
    "new_test_labels  = np.array([ex[\"label\"] for ex in new_data[\"test\"]], dtype=np.int64)\n",
    "new_label_names  = new_data[\"train\"].features[\"label\"].names\n",
    "\n",
    "def compare_lists(a, b, name, show_mismatches=5):\n",
    "    same_len = len(a) == len(b)\n",
    "    equal_all = same_len and all(x == y for x, y in zip(a, b))\n",
    "    print(f\"{name}: length old={len(a)}, new={len(b)}; identical={equal_all}\")\n",
    "    if not equal_all:\n",
    "        print(\" Showing up to\", show_mismatches, \"differences (index, old_preview, new_preview):\")\n",
    "        cnt = 0\n",
    "        for i, (x, y) in enumerate(zip(a, b)):\n",
    "            if x != y:\n",
    "                print(i, repr(x)[:120], \"...\", repr(y)[:120])\n",
    "                cnt += 1\n",
    "                if cnt >= show_mismatches:\n",
    "                    break\n",
    "        # if lengths differ, show tail/extra items\n",
    "        if len(a) != len(b):\n",
    "            if len(a) > len(b):\n",
    "                print(\"Extra items in old at indices:\", list(range(len(b), min(len(a), len(b)+show_mismatches))))\n",
    "            else:\n",
    "                print(\"Extra items in new at indices:\", list(range(len(a), min(len(b), len(a)+show_mismatches))))\n",
    "\n",
    "# Compare texts and labels\n",
    "compare_lists(old_train_texts, new_train_texts, \"Train texts\")\n",
    "compare_lists(old_test_texts,  new_test_texts,  \"Test  texts\")\n",
    "\n",
    "print(\"Train labels identical:\", np.array_equal(old_train_labels, new_train_labels))\n",
    "print(\"Test  labels identical:\", np.array_equal(old_test_labels,  new_test_labels))\n",
    "\n",
    "print(\"Label names old:\", old_label_names)\n",
    "print(\"Label names new:\", new_label_names)\n",
    "print(\"Label names identical:\", old_label_names == new_label_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 3 - Getting Fasttext Accuracy\n",
    "\n",
    "Now that we have a way of training both a fasttext character model, and a fasttext word model, we would like a function that goes through our test texts and labels, and outputs the fasttext model's accuracy on the given test set\n",
    "\n",
    "#### **3.1. ðŸ’» Implement the below function to get the accuracy of a fasttext model. It should return the accuracy of the fasttext model when trying to predict each of the four labels, as well as the average accuracy across all labels. You can use the test_prediction function above to get predictions, but you can also implement your own method.**\n",
    "\n",
    "#### **3.2. There shouldn't be *that* big of a difference in performance between the character model and word model when predicting the genres of these texts... why do you think that is?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "There's a rather big difference between the two models, both in computation and accuracy. whilst the word model is very quick, it also has worse accuracy. the word model takes about 0.15 seconds to complete, whilst the character model takes upwards 1.5 seconds, but it has a greater accuracy, which is what we would expect from this model since it's more robust to gramatical errors and inflections.\n",
    "so we found that there is a rather big difference in performance (accuracy/computation) \n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fasttext word model...\n",
      "({'World': 0.7626315789473684, 'Sports': 0.8315789473684211, 'Business': 0.8131578947368421, 'Sci/Tech': 0.7552631578947369, 'average_accuracy': 0.7906578947368421}, 'Model testing completed in 0.22 seconds')\n",
      "\n",
      "Testing fasttext character model...\n",
      "({'World': 0.8578947368421053, 'Sports': 0.9468421052631579, 'Business': 0.8563157894736843, 'Sci/Tech': 0.86, 'average_accuracy': 0.8802631578947369}, 'Model testing completed in 1.51 seconds')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def test_fasttext_model(fasttext_model, test_texts, test_labels, label_names):\n",
    "    start_time = time.time()\n",
    "    \"\"\"\n",
    "    Tests FastText model accuracy on each label and overall.\n",
    "    Works with numeric test_labels (0â€“3) and FastText string labels.\n",
    "    \"\"\"\n",
    "    # Initialize counters. Supports label_names as array-like of strings\n",
    "    # or dict mapping int->str (in which case iterating yields keys -> ints).\n",
    "    # We want keys to be the label names for per-label counters.\n",
    "    if isinstance(label_names, dict):\n",
    "        names_iter = label_names.values()\n",
    "    else:\n",
    "        names_iter = label_names\n",
    "\n",
    "    correct_per_label = {name: 0 for name in names_iter}\n",
    "    total_per_label = {name: 0 for name in names_iter}\n",
    "\n",
    "    for text, true_label in zip(test_texts, test_labels):\n",
    "        true_label_name = label_names[int(true_label)]  # convert 0â†’\"World\" (works for ndarray or dict)\n",
    "        pred_label = fasttext_model.predict(text)[0][0]  # \"__label__Business\"\n",
    "        pred_label_name = pred_label.replace(\"__label__\", \"\")\n",
    "\n",
    "        total_per_label[true_label_name] += 1\n",
    "        if pred_label_name == true_label_name:\n",
    "            correct_per_label[true_label_name] += 1\n",
    "\n",
    "    # Compute per-label accuracies safely (avoid div-by-zero)\n",
    "    accuracies = {}\n",
    "    for name in total_per_label:\n",
    "        if total_per_label[name] > 0:\n",
    "            accuracies[name] = correct_per_label[name] / total_per_label[name]\n",
    "        else:\n",
    "            accuracies[name] = 0.0\n",
    "\n",
    "    # Overall (weighted) accuracy across all examples\n",
    "    total_correct = sum(correct_per_label.values())\n",
    "    total_examples = sum(total_per_label.values())\n",
    "    accuracies[\"average_accuracy\"] = total_correct / total_examples if total_examples > 0 else 0.0\n",
    "    end_time = time.time()\n",
    "    return accuracies, f\"Model testing completed in {end_time - start_time:.2f} seconds\"\n",
    "\n",
    "\n",
    "print(\"Testing fasttext word model...\")\n",
    "print(test_fasttext_model(fasttext_word_model, test_texts, test_labels, ag_news_labels))\n",
    "\n",
    "print(\"\\nTesting fasttext character model...\")\n",
    "print(test_fasttext_model(fasttext_char_model, test_texts, test_labels, ag_news_labels))\n",
    "print() # tak chat, nu har jeg det meget beere. det var sÃ¥ lidt *maskingevÃ¦rdslyde*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "## Part 3 - GloVe to create embeddings vectors\n",
    "\n",
    "*[GloVe Paper here](https://aclanthology.org/D14-1162.pdf), [GloVe Project page here](https://nlp.stanford.edu/projects/glove/)*\n",
    "\n",
    "GloVe is called a \"global log-bilinear regression model\" which combines the strengths of global matrix factorization and local context window methods.\n",
    "\n",
    "In plain English, this means it combines methods that work by collecting information on the entire corpus, with other methods that capture more local patterns, essentially what we see with Fasttext that considers local n-grams. GloVe just considers \"context windows\" rather than an n-gram. Overall, what they want are nicely defined, linear relationships, decided by comparing the co-occurences of different words.\n",
    "\n",
    "The selling point really, is that while a run-of-the-mill neural network **may** be able to answer the questions: \"Skibidi is to Toilet as Fanum is to ...?\", it will not necessarily be able to do it in a linear manner. Therefore considering all the word vectors together in their latent space, may not yield good information. GloVe fixes this by keeping all vector substructures linear.\n",
    "\n",
    "Essentially, GloVe trains by mixing a [Skipgram model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) (just a neural network) with a function that works more on the entire corpus, while maintaining a weighting between the two. Because GloVe works best on huge corpora of data, we are not going to train it ourselves, but just use pretrained GloVe vectors, collected from their [project page](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 4  - GloVe Theoretical questions\n",
    "\n",
    "\n",
    "#### **4.1. On their project page, GloVe gives a few different possibilities for GloVe vectors, including ones with embedding dimension 50, 100, 200, and 300. What potential downsides and upsides are there to larger/smaller embedding dimensions? Consider both training and subsequent usage.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Higher dimension embeddings occasionally overestimate the semantic similarity between antonyms(antiwords). in general:\n",
    "higher dimension; more representational power, but risk of overfitting  and higher cost. smaller dimensions: faster, cheaper and decent for small tasks, risk of underfitting. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.2. Also on their webage, they give two options for the \"Common Crawl\" GloVe vector set: \"42B tokens, 1.9M vocab, 300d vectors\" and \"840B tokens, 2.2M vocab, 300d vectors\". What are the differences between these?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Common Crawl is a large dataset gathered from the internet. The difference between the two sets is mostly the data size (token amount). An important detail to note is however that the text was not lowercased for the larger vocabulary, making the results not directly comparable. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.3. GloVE is presented purely as an embedding model, without any sort of infrastructure to support classification, generation, or anything of the sort. Why is such an only-embedding model still valuable??**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Because it's an effectient method of vectorizing the corpus, and Ã­s a great base for creating classification models and all that. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.4. In the GloVe paper, they say they attempt to model $F(w_i, w_j, \\tilde{w_k}) = \\frac{P_{ik}}{P_jk}$. That is, the probability of one word given another, compared to the probability of that same word given a third word. For this, they briefly consider using a neural network as the function $F$ but decide against it, as \"doing so would obfuscate the linear strcutures we are trying to capture\", what linear structures are talked about and how would they be obfuscated by using something like a neural network?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "a NN would \"obfuscate\" the data, because it uses non-linear transformations, and that would make the semantic relationsships non-linear and complicated. Non-linearity would maje it hard to directly interpret the learned vector differences as meaningful direction, we would lose the simple linear structure, that allows us to use vector maths to express relations\n",
    "\n",
    "\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #00695C; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "`load_glove` and `create_GloVE_vector` below, are functions to load and create convenient data structures from our GloVE file, and create a combined GLoVE vector from a longer text in much the same way FastText does, respectively.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path):\n",
    "    \"\"\"\n",
    "    Loads GloVe vectors robustly, trying utf-8 then falling back to latin-1.\n",
    "    Returns: dict[word] = np.ndarray(float32)\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    encodings = ['utf-8', 'latin-1']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(glove_path, 'r', encoding=enc, errors='strict') as f:\n",
    "                print(f\"Creating GloVE dictionary using encoding: {enc}\")\n",
    "                for line in tqdm(f):\n",
    "                    parts = line.rstrip().split(' ')\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    word = parts[0]\n",
    "                    vec = np.asarray(parts[1:], dtype='float32')\n",
    "                    glove[word] = vec\n",
    "            return glove\n",
    "        except UnicodeDecodeError:\n",
    "            # try next encoding\n",
    "            continue\n",
    "        except Exception:\n",
    "            # if other errors occur, try a permissive open with replace\n",
    "            break\n",
    "\n",
    "    # last-resort permissive open\n",
    "    with open(glove_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        print(\"Creating GloVE dictionary using permissive open (errors='replace').\")\n",
    "        for line in tqdm(f):\n",
    "            parts = line.rstrip().split(' ')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            vec = np.asarray(parts[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "\n",
    "    return glove\n",
    "\n",
    "def create_GloVE_vector(text, glove):\n",
    "    \"\"\"\n",
    "    Creates a GloVe vector for a longer text by averaging word vectors.\n",
    "    Returns a vector of same shape as glove entries (or zeros if none found).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z0-9 ]+', '', text)\n",
    "    words = text.split()\n",
    "\n",
    "    # If glove is empty, raise informative error\n",
    "    if not glove:\n",
    "        raise ValueError(\"`glove` dictionary is empty. Load GloVe first with `load_glove`.\")\n",
    "\n",
    "    placeholder = np.zeros_like(next(iter(glove.values())))\n",
    "    vectors = [glove[word] for word in words if word in glove]\n",
    "\n",
    "    if not vectors:\n",
    "        return placeholder.copy()\n",
    "\n",
    "    # Stack vectors and compute mean across axis=0\n",
    "    avg = np.mean(np.stack(vectors, axis=0), axis=0)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GloVE dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:05, 71917.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe dictionary, doing it here since we only wanna do it once, since it takes a fuckton of time\n",
    "glove = load_glove('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "\n",
    "### Exercise 5 - Word similarity\n",
    "\n",
    "#### **5.1. ðŸ’» Complete the `word_similarity` function below. It should compute the cosine simliarity between two provided word embedding vectors.**\n",
    "\n",
    "\n",
    "#### **5.2. Briefly comment on the similarities obtained when running the cell two spaces below**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "GloVe similarity between cat and dog is 0.921800434589386\n",
    "GloVe similarity between cat and banana is 0.3396517336368561\n",
    "GloVe similarity between cat and cat is 0.9999998807907104\n",
    "GloVe similarity between camera and man is 0.47356218099594116\n",
    "GloVe similarity between steel and beams is 0.5590428709983826\n",
    "GloVe similarity between six and 6 is 0.7447779774665833\n",
    "\n",
    "most of the similarities makes sense in their relation. though its funny how six and 6 are not that similar, but that must be because of the context in which we use the two methods of typing out \"six/6\" \n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(vec1, vec2, glove=None):\n",
    "    \"\"\"\n",
    "    Gets the cosine similarity between two vectors or two words in the GloVE dictionary\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray, str): First vector to compare to the second\n",
    "        vec2 (np.ndarray, str): Second vector to compare to the first\n",
    "        glove (dict): GloVe dictionary if we want to compare words instead of just vectors, else None\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity of the two vectors or words\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vectors from the GloVE dictionary if the input is a string\n",
    "    if glove is not None:\n",
    "        vec1, vec2 = glove[vec1], glove[vec2]\n",
    "    \n",
    "    # Return the cosine similarity of the two vectors\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe similarity between cat and dog is 0.921800434589386\n",
      "GloVe similarity between cat and banana is 0.3396517336368561\n",
      "GloVe similarity between cat and cat is 0.9999998807907104\n",
      "GloVe similarity between camera and man is 0.47356218099594116\n",
      "GloVe similarity between steel and beams is 0.5590428709983826\n",
      "GloVe similarity between six and 6 is 0.7447779774665833\n"
     ]
    }
   ],
   "source": [
    "# Test the word simliarity function on some word pairs\n",
    "\n",
    "word_pairs = [('cat', 'dog'), ('cat', 'banana'), ('cat', 'cat'), ('camera', 'man'), ('steel', 'beams'), ('six', '6')]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    print(f\"GloVe similarity between {word1} and {word2} is {word_similarity(word1, word2, glove)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 6 - Comparing word embeddings vectors\n",
    "\n",
    "Since both GloVe and Fasttext, at their core, are both methods for egenerating embedding vectors, it would make sense to examine how they look compared to one another. Since both live in high-dimensional spaces, we must perform PCA on them to actually make sense of them in a graphical way.\n",
    "\n",
    "#### **6.1 Explain in a manner you find most fitting: *What are word embeddings?***\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **6.2. Explain shortly what you expect to find if we perform PCA on the matrix of word-embeddings, that is the matrix which holds a vector representation of each word in our vocabulary**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "If we run PCA, weâ€™ll squeeze the high-dimensional embeddings into 2D. In theori we should see that similar words ending up close to each other, and The greater the difference in words the more spread out they will be. Basically, PCA helps us visualize the structure of the embedding space.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **6.3. When getting vectors for all words in a large text, GloVe should be significantly faster than Fasttext, why is this?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Word embeddings are numbers that represent words. Each word gets a vector, and words that mean similar things end up close to each other in this vector space. since computers can not prossece letters and words then it is a way for computers to understand relationships between words.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **6.4. Which model (GloVe or fasttext) do you think performs best at seperating the four classes of the AG News dataset if we get average embedding vectors for each text in the training and testing dataset, perform PCA on these and plot them on the two top principal components. And why do you think that model performs best at seperating the four classes?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "FastText will probably separate the classes better.\n",
    "Thatâ€™s because FastText understands words through their subword parts, this makes it so that it handles rare words, names, and technical terms better, because all of thise types of words show up a lot in news articles. When you average these richer vectors across a whole text and then do PCA, the categories tend to be easier to tell apart.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 7 - PCA on word embeddings\n",
    "\n",
    "We now examine how the words conatined in the four different classes look when projected unto a 2-d space.\n",
    "\n",
    "\n",
    "#### **7.1 Run the code in the two cells below, what does it do overall? What kind of plots does it produce?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "GloVe is fast because each word already has its own vector stored, therfore you just look it up.\n",
    "FastText is slower because it builds a wordâ€™s vector from lots of smaller pieces. This takes extra computation every time you ask for a wordâ€™s embedding.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **7.2 Why would we be interested in making exactly these plots?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **7.3 Change the `current_model` from `glove` to either `fasttext_char_model` or `fasttext_word_model`, does the result change? If so, how?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = glove\n",
    "# current_model_name = fasttext_char_model\n",
    "# current_model = fasttext_word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = []\n",
    "feats_test = []\n",
    "\n",
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleaning text of non-alphanumerics using the aforementioned string translation table\n",
    "    Just for convenience in regards to GloVe and useful word vectors\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator).lower()\n",
    "\n",
    "def get_average_embedding_vectors(model, train_texts, test_texts):\n",
    "    print(\"Getting fasttext average embedding vectors for each class...\")\n",
    "    placeholder = np.zeros_like(glove['the'])\n",
    "\n",
    "    for text in tqdm(train_texts):\n",
    "        words = clean(text).split()\n",
    "        if getattr(model, 'get', False):\n",
    "            feats_train.append(np.mean([model.get(word, placeholder) for word in words], axis=0))\n",
    "        else:\n",
    "            feats_train.append(np.mean([model.get_word_vector(word) for word in words], axis=0))\n",
    "\n",
    "    # Same but for each text in test set\n",
    "    for text in tqdm(test_texts):\n",
    "        words = clean(text).split()\n",
    "        if getattr(model, 'get', False):\n",
    "            feats_test.append(np.mean([model.get(word, placeholder) for word in words], axis=0))\n",
    "        else:\n",
    "            feats_test.append(np.mean([model.get_word_vector(word) for word in words], axis=0))\n",
    "\n",
    "    return np.array(feats_train), np.array(feats_test)\n",
    "\n",
    "feats_train, feats_test = get_average_embedding_vectors(current_model, train_texts, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(n_components, feats_train, feats_test, train_labels, test_labels, label_names):\n",
    "    \"\"\"\n",
    "    Does PCA with n_components on given features and plots the result in two dimensions\n",
    "\n",
    "    Args:\n",
    "        n_components (int): How many components to use in the PCA\n",
    "        feats_train (np.ndarray): Average embedding vectors for each text in the training set\n",
    "        feats_test (np.ndarray): Average embedding vectors for each text in the test set\n",
    "        train_labels (np.ndarray): Labels for each text in the training set\n",
    "        test_labels (np.ndarray): Labels for each text in the test set\n",
    "        label_names (dict): Dictionary of int: str so that int corresponds to the label name\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # fit_transform avoids having to manually transform the vectors with the matrix afterwards\n",
    "    Vtrain = pca.fit_transform(feats_train)\n",
    "    Vtrain_var_explained = pca.explained_variance_ratio_\n",
    "\n",
    "    Vtest = pca.fit_transform(feats_test)\n",
    "    Vtest_var_explained = pca.explained_variance_ratio_\n",
    "\n",
    "    colors = 'rgbk'\n",
    "    for label, transformed_vector, title in zip([train_labels, test_labels], [Vtrain, Vtest], [f'Training set var_explained: {sum(Vtrain_var_explained[:2])}', f'Test set, var_explained: {sum(Vtest_var_explained[:2])}']):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 0], transformed_vector[label==i, 1], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 1], transformed_vector[label==i, 2], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC2')\n",
    "        plt.ylabel('PC3')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        for i in range(4):\n",
    "            plt.plot(transformed_vector[label==i, 2], transformed_vector[label==i, 3], '.', color=colors[i], label=label_names[i])\n",
    "        plt.legend()\n",
    "        plt.xlabel('PC3')\n",
    "        plt.ylabel('PC4')\n",
    "        plt.suptitle(title, fontweight='bold', fontsize=15)\n",
    "\n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(range(n_components)),np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.title(\"Explained variance compared to number of components\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"Explained variance\")\n",
    "    plt.show()\n",
    "    \n",
    "    return pca\n",
    "\n",
    "n_components = 50\n",
    "pca_word_vec = plot_pca(n_components, feats_train, feats_test, train_labels, test_labels, ag_news_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 8 - Getting PCA vectors of single words\n",
    "\n",
    "Previously, we considered the whole space of words together when projected unto 2 principal components. We can also be interested in doing this with just single words...\n",
    "\n",
    "\n",
    "#### **8.1 ðŸ’» Complete the function `get_vector_transform` below to get a word vector a word and project it to a given PCA with n principal components**\n",
    "\n",
    "\n",
    "#### **8.2. Comment on the plot created two cells below. Do the vectors correspond to what you'd expect?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.3. What must we be cognisant of when doing PCA plots like this on $n$ principal components?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **8.4 Run the cell three steps below to compare different words to the previous list of words. How do they compare? Do these comparisons change significantly when you run with more principal components?**\n",
    "\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_transform(word, model, pca, n=2):\n",
    "    \"\"\"\n",
    "    Gets the PCA transformed vector of a given word in the model with n principal components\n",
    "\n",
    "    Args:\n",
    "        word (str): word to first get the embedding vector of and then project to PCA space\n",
    "        model (dict, fasttext model): Either a fasttext model or GloVe dictionary\n",
    "        pca (sklearn.decomposition.PCA): PCA for the model given, should be the same model as the one used to get the vectors\n",
    "        n (int, optional): Number of princpal components to project to. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Word vector of word projected to PCA space with n principal components\n",
    "    \"\"\"\n",
    "    # Get the word vector from the model (glove or Fasttext)\n",
    "    word_vec = model[word]\n",
    "    # Reshape so it fits with PCA transform\n",
    "    word_vec = word_vec.reshape(1, -1)\n",
    "\n",
    "    # Return the PCA transformed vector with n principal components\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['company', 'business', 'cat', 'software', 'microsoft', 'mouse']\n",
    "\n",
    "transformed_word_vectors = [get_vector_transform(i, current_model, pca_word_vec, n=2) for i in words]\n",
    "plt.figure(figsize=(28, 10))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for idx, vec in enumerate(transformed_word_vectors):\n",
    "    plt.plot([0, vec[0]], [0, vec[1]], 'b--')\n",
    "    circle = plt.Circle((vec[0], vec[1]), radius=0.015)\n",
    "    ax.add_patch(circle)\n",
    "    label = ax.annotate(words[idx], xy=(vec[0], vec[1]), fontsize=20, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.xlabel('Latent Semantic dim 1')\n",
    "plt.ylabel('Latent Semantic dim 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "to_compare = ['software', 'business', 'world'] # Three words, that should be labeled as three different things\n",
    "for word in words:\n",
    "    for comparison in to_compare:\n",
    "        distance = word_similarity(\n",
    "                                    vec1=get_vector_transform(word, current_model, pca_word_vec, n=2),\n",
    "                                    vec2=get_vector_transform(comparison, current_model, pca_word_vec,n=2),\n",
    "                                    glove=None\n",
    "                                    )\n",
    "        print(f\"{word}-{comparison}: {distance}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 9 - Deliberately fucking up our text\n",
    "\n",
    "FastText was presented as somewhat gracefully handling OOV words, particularly those spelled wrong.\n",
    "\n",
    "Below, we define the `dyslexibot` function, which deliberately replaces a certian percentage of characters in our test set with either other random letters, or all kinds of random signs. \n",
    "\n",
    "#### **9.1 ðŸ’» Test both the fasttext_word_model and the fasttext_char_model on text generated by dyslexibot. Change the $p$ value and perhaps the $\\text{extra scuffed}$ option. Try to make the fasttext_word_model as bad as possible while the fasttext_char_model still keeps somewhat good performance. Comment on what you did to achieve this.**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dyslexibot(test_set: list, p=0.05, extra_scuffed=False):\n",
    "    \"\"\"\n",
    "    tHe AlMiGhTy dyslexibot(tm) replaces letters with probability p\n",
    "    extra_scuffed does what it says: it makes the replacements even harder to guess\n",
    "    This function is pretty ineffective, if you want to spruce it up, you are welcome to do so\n",
    "    \"\"\"\n",
    "\n",
    "    if extra_scuffed:\n",
    "        test_set_letters = np.array(list(set(''.join(test_texts)))) # Can replace with all letters currently in test set\n",
    "    else:\n",
    "        test_set_letters = np.array(list(string.ascii_lowercase)) # Can only replace with lowercase letters\n",
    "\n",
    "    new_test_set = [text.split(' ') for text in test_set.copy()]\n",
    "\n",
    "    print(\"Replacing text\")\n",
    "    for i, text in tqdm(enumerate(new_test_set)):\n",
    "        for r, word in enumerate(text):\n",
    "            word = list(word)\n",
    "            for t, letter in enumerate(word):\n",
    "                rand = random.uniform(0, 1)\n",
    "\n",
    "                if extra_scuffed and rand < p: # We replace even spaces!\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "                elif letter != ' ' and rand < p:\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "            text[r] = ''.join(word)\n",
    "        new_test_set[i] = ' '.join(text)\n",
    "    return np.array(new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing that dyslexibot works\n",
    "text = [\"have you heard of the tragedy of darth plagueis the wise\"]\n",
    "print(dyslexibot(text, p=0.10, extra_scuffed=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dyslexitext = dyslexibot(test_texts, p=0.20, extra_scuffed=False)\n",
    "\n",
    "# Insert your training loop here to calculate the test accuracy on the dyslexitext\n",
    "# using both the word-wise fasttext and the character-wise fasttext\n",
    "\n",
    "\n",
    "print(\"Testing fasttext word model...\")\n",
    "test_fasttext_model(fasttext_word_model, dyslexitext, test_labels, ag_news_labels)\n",
    "\n",
    "print(\"\\nTesting fasttext character model...\")\n",
    "test_fasttext_model(fasttext_char_model, dyslexitext, test_labels, ag_news_labels)\n",
    "print()\n",
    "\n",
    "\n",
    "# # TODO: What it says one line above\n",
    "# ???????????\n",
    "# ???????????\n",
    "# ???????????\n",
    "# raise NotImplementedError(\"Test me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #E6E6E6;\">\n",
    "<span style=\"background-color: #545454; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "### Exercise 10 - Custom text classification\n",
    "\n",
    "Finally, because it's fun, (and to show the robustness of fasttext), we can try to make our text and let fasttext classify this as one of the four classes.\n",
    "\n",
    "Rememeber that the original dataset used texts of around 240 words, so you can either experiment with texts shorter or longer than this and see whether the accuracy is significantly different.\n",
    "\n",
    "As an extra challenge, you can try to create a text which is as close to the model's decision boundary as possible, IE. one that is as close as possible to being classified as either two or more of the classes\n",
    "\n",
    "#### **10.1 What text did you make, and why do you think it was classifed the way it was?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_text = \"\"\n",
    "\n",
    "assert own_text != \"\", \"Come on, be creative\"\n",
    "\n",
    "print(\"The text is classified as: \", fasttext_word_model.predict(own_text))#[0][0][9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  $\\star$ $\\star$ $\\star$ Exercise 6 Extremely optional task\n",
    "\n",
    "**1. It's quite a waste to have all those juicy principle components of the embedding matrices without using them in a classifier of some sort, right?**\n",
    "\n",
    "**Create a classifier to classify texts as one of the four labels, based on the projections of their words unto a number of the principle components of the embedding matrices. Compare this classifier to the fasttext classifier and reflect on their performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Asking for a whole classifier is a bit much, no?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\star$ **2.5. As mentioned, attention-based models fix a lot of issues that older Word2Vec models had. Particularly, they do not need N-grams to capture context information. Why is this?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### **4.4. Given just a bunch of embedding vectors in a vacuum from a known GloVe embedding, it is usually not possible to get a 1-1 correspondance of what words these vectors were. Why is this?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### **4.4. How would you most closely estabilsh this 1-1 correspondance between given vectors from a known GloVe embedding and their corresponding words?**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "# TODO: Consider if this makes sense!\n",
    "\n",
    "# Replace wiht question along the lines of \"Why is this not the most cognisant thing to do?\"\n",
    "\n",
    "#### $\\star$ **4.5. Why can't we just use a Python dictionary with vectors as keys and words as values? (essentially a reverse GloVe dictionary)**\n",
    "\n",
    "<span style=\"background-color: #00590D; padding:8px; display:block; border-left:4px solid #4682b4\">\n",
    "\n",
    "Your answer here $\\dots$\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\star$ 3.2 GloVe vector word\n",
    "\n",
    "**1. Implement the function below which, given a GloVe vector for a word, finds what word it was before it was embedded**\n",
    "\n",
    "**2. Slowly add more words to the 'words' list two cells below. At what point do you reckon a text, transformed to GloVe vectors becomes too long to lookup words for each vector in it?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**$\\star \\star$ 3. Why is this method so slow? What factors influence its speed? How could you levearage things like asynchronous execution or multithreading to speed it up?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**$\\star \\star \\star \\star \\star$ Implment this asynchronous execution or multithreading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word(target_vector, glove_lookup):\n",
    "    \"\"\"\n",
    "    Finds the closest word in the GloVE dictionary to a given vector\n",
    "\n",
    "    Args:\n",
    "        target_vector (np.ndarray): Vector to find the closest word to\n",
    "        glove_lookup (dict): GloVE dictionary to look up the closest word in \n",
    "\n",
    "    Returns:\n",
    "        str: Word in keys of glove_lookup closest to target_vector\n",
    "    \"\"\"\n",
    "    # Define an initial max similarity and closest word (both to be updated)\n",
    "    ...\n",
    "\n",
    "    # Iterate over all words in the GloVE dictionary and find the most similar one to the target vector\n",
    "    ...\n",
    "    \n",
    "\n",
    "    # If the similarity is higher than the current max, update the max similarity and closest word\n",
    "    ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['cat']\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word_vector = glove[word]\n",
    "    closest_word = find_closest_word(word_vector, glove)\n",
    "    print(f\"Closest word found to actual word {words[i]}:\", closest_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
